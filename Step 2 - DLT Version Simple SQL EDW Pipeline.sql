-- Databricks notebook source
-- MAGIC %md
-- MAGIC 
-- MAGIC # This notebook generates a full data pipeline from databricks dataset - iot-stream
-- MAGIC 
-- MAGIC ## This creates 2 tables: 
-- MAGIC 
-- MAGIC <b> Database: </b> iot_dashboard
-- MAGIC 
-- MAGIC <b> Tables: </b> silver_sensors, silver_users 
-- MAGIC 
-- MAGIC <b> Params: </b> StartOver (Yes/No) - allows user to truncate and reload pipeline

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC 
-- MAGIC ## This is built as a library for a Delta Live Tables pipeline

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Exhaustive list of all cloud_files STREAMING LIVE TABLE options
-- MAGIC https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-incremental-data.html#language-sql

-- COMMAND ----------

-- DBTITLE 1,Incrementally Ingest Source Data from Raw Files
--No longer need a separate copy into statement, you can use the Databricks Autoloader directly in SQL by using the cloud_files function
-- OPTIONALLY defined DDL in the table definition
CREATE OR REFRESH STREAMING LIVE TABLE bronze_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING,
CONSTRAINT has_device EXPECT (device_id IS NOT NULL) ON VIOLATION DROP ROW  ,
CONSTRAINT has_user EXPECT(user_id IS NOT NULL) ON VIOLATION DROP ROW,
CONSTRAINT has_data EXPECT(num_steps IS NOT NULL) -- with no violation rule, nothing happens, we just track quality in DLT
)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="create_timestamp,device_id,user_id",
"pipelines.trigger.interval"="1 hour")
AS 
SELECT 
id::bigint AS Id,
device_id::integer AS device_id,
user_id::integer AS user_id,
calories_burnt::decimal(10,2) AS calories_burnt, 
miles_walked::decimal(10,2) AS miles_walked, 
num_steps::decimal(10,2) AS num_steps,
timestamp::timestamp AS timestamp,
value AS value
FROM cloud_files("/databricks-datasets/iot-stream/data-device/", "json");

-- To make incremental - Add STREAMING keyword before LIVE TABLE

-- COMMAND ----------

-- MAGIC %md
-- MAGIC 
-- MAGIC ## Process Change data with updates or deletes 
-- MAGIC API Docs: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cdc.html
-- MAGIC 
-- MAGIC 
-- MAGIC ### Automatically store change as SCD 1 or SCD 2 Type changes
-- MAGIC 
-- MAGIC SCD 1/2 Docs: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cdc.html#language-sql

-- COMMAND ----------

-- DBTITLE 1,Incremental upsert data into target silver layer
-- Create and populate the target table.
CREATE OR REFRESH STREAMING LIVE TABLE silver_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"quality"="silver",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="create_timestamp,device_id,user_id",
"pipelines.trigger.interval"="1 hour"
);

-- COMMAND ----------

-- DBTITLE 1,Actually run CDC Transformation Operation
APPLY CHANGES INTO
  LIVE.silver_sensors
FROM
  STREAM(LIVE.bronze_sensors) -- use STREAM to get change feed, use LIVE to get DAG source table
KEYS
  (Id, user_id, device_id) -- Identical to the ON statement in MERGE, can be 1 of many keys
APPLY AS DELETE WHEN
  operation = "DELETE"
SEQUENCE BY
  timestamp
COLUMNS * EXCEPT
   (device_id, user_id, Id) -- Do not update these
--    Optionally exclude columns like metadata or operation types, by default, UPDATE * is the operation
STORED AS
  SCD TYPE 1 -- [SCD TYPE 2] will expire updated originals

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC 
-- MAGIC ## FULL REFRESH EXAMPLE - Ingest Full User Data Set Each Load

-- COMMAND ----------

-- DBTITLE 1,FulltIngest Raw User Data
CREATE OR REPLACE LIVE TABLE silver_users
(
userid BIGINT GENERATED BY DEFAULT AS IDENTITY,
gender STRING,
age INT,
height DECIMAL(10,2), 
weight DECIMAL(10,2),
smoker STRING,
familyhistory STRING,
cholestlevs STRING,
bp STRING,
risk DECIMAL(10,2),
update_timestamp TIMESTAMP,
CONSTRAINT has_user EXPECT (userid IS NOT NULL) ON VIOLATION DROP ROW
)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"quality"="silver",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="userid",
"pipelines.trigger.interval"="1 day"
)
AS (SELECT 
userid::bigint AS userid,
gender AS gender,
age::integer AS age,
height::decimal(10,2) AS height, 
weight::decimal(10,2) AS weight,
smoker AS smoker,
familyhistory AS familyhistory,
cholestlevs AS cholestlevs,
bp AS bp,
risk::decimal(10,2) AS risk,
current_timestamp() AS update_timestamp
FROM cloud_files("/databricks-datasets/iot-stream/data-user/", map('format', 'csv', 'header', 'true'))
)
;
