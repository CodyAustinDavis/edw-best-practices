# Databricks notebook source
# MAGIC %pip install -r helperfunctions/requirements.txt

# COMMAND ----------

from helperfunctions.dbsqlclient import ServerlessClient
from helperfunctions.transactions import Transaction, TransactionException


class DBSQLTransactionManager(Transaction):

  def __init__(self, warehouse_id, mode="selected_tables", uc_default=False, host_name=None, token=None):

    super().__init__(mode=mode, uc_default=uc_default)
    self.host_name = host_name
    self.token = token
    self.warehouse_id = warehouse_id

    return 
  

### Execute multi statment SQL, now we can implement this easier for Serverless or not Serverless
def execute_sql_transaction(self, sql_string, tables_to_manage=[], force=False, return_type="message"):

  ## return_type = message (returns status messages), last_result (returns the result of the last command in the sql chain)
  ## If force= True, then if transaction manager fails to find tables, then it runs the SQL anyways
  ## You do not NEED to run SQL this way to rollback a transaction,
  ## but it automatically breaks up multiple statements in one SQL file into a series of spark.sql() commands

  serverless_client = ServerlessClient(warehouse_id = self.warehouse_id, token=self.token, host_name=self.host_name) ## token=<optional>, host_name=<optional>verbose=True for print statements and other debugging messages
  
  stmts = sql_string.split(";")

  ## Save to class state
  self.raw_sql_statement = sql_string
  self.sql_statement_list = stmts

  success_tables = False

  try:
    self.begin_dynamic_transaction(tables_to_manage=tables_to_manage)

    success_tables = True

  except Exception as e:
    print(f"FAILED: failed to acquire tables with errors: {str(e)}")
  
  ## If succeeded or force = True, then run the SQL
  if success_tables or force:
    if success_tables == False and force == True:
      warnings.warn("WARNING: Failed to acquire tables but force flag = True, so SQL statement will run anyways")

    ## Run the Transaction Logic with Serverless Client
    try:
      
      print(f"TRANSACTION IN PROGRESS ...Running multi statement SQL transaction now\n")

      ## OPTION 1: return status message
      if return_type == "message":

        result_df = serverless_client.submit_multiple_sql_commands(sql_statements=sql_string)

      elif return_type == "last_result":
        
        result_df = serverless_client.submit_multiple_sql_commands_last_results(sql_statements=sql_string)

      else:
        result_df = None
        print("No run mode selected, select 'message' or 'last_results'")


      print(f"\n TRANSACTION SUCCEEDED: Multi Statement SQL Transaction Successfull! Updating Snapshot\n ")
      self.commit_transaction()


      ## Return results after committing sucesss
      return result_df
    
        
    except Exception as e:
      print(f"\n TRANSACTION FAILED to run all statements... ROLLING BACK \n")
      self.rollback_transaction()
      print(f"Rollback successful!")
      
      raise(e)

  else:

    raise(TransactionException(message="Failed to acquire tables and force=False, not running process.", errors="Failed to acquire tables and force=False, not running process."))
    

# COMMAND ----------

# DBTITLE 1,Example Inputs For Client
token = None ## optional
host_name = None ## optional
warehouse_id = "475b94ddc7cd5211"

# COMMAND ----------

# DBTITLE 1,Example Multi Statement Transaction
sqlString = """
USE CATALOG hive_metastore;

CREATE SCHEMA IF NOT EXISTS iot_dashboard;

USE SCHEMA iot_dashboard;

-- Create Tables
CREATE OR REPLACE TABLE iot_dashboard.bronze_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING
)
USING DELTA
TBLPROPERTIES("delta.targetFileSize"="128mb");

CREATE OR REPLACE TABLE iot_dashboard.silver_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING
)
USING DELTA 
PARTITIONED BY (user_id)
TBLPROPERTIES("delta.targetFileSize"="128mb");

-- Statement 1 -- the load
COPY INTO iot_dashboard.bronze_sensors
FROM (SELECT 
      id::bigint AS Id,
      device_id::integer AS device_id,
      user_id::integer AS user_id,
      calories_burnt::decimal(10,2) AS calories_burnt, 
      miles_walked::decimal(10,2) AS miles_walked, 
      num_steps::decimal(10,2) AS num_steps, 
      timestamp::timestamp AS timestamp,
      value AS value -- This is a JSON object
FROM "/databricks-datasets/iot-stream/data-device/")
FILEFORMAT = json
COPY_OPTIONS('force'='true') -- 'false' -- process incrementally
--option to be incremental or always load all files
; 

-- Statement 2
MERGE INTO iot_dashboard.silver_sensors AS target
USING (SELECT Id::integer,
              device_id::integer,
              user_id::integer,
              calories_burnt::decimal,
              miles_walked::decimal,
              num_steps::decimal,
              timestamp::timestamp,
              value::string
              FROM iot_dashboard.bronze_sensors) AS source
ON source.Id = target.Id
AND source.user_id = target.user_id
AND source.device_id = target.device_id
WHEN MATCHED THEN UPDATE SET 
  target.calories_burnt = source.calories_burnt,
  target.miles_walked = source.miles_walked,
  target.num_steps = source.num_steps,
  target.timestamp = source.timestamp
WHEN NOT MATCHED THEN INSERT *;

OPTIMIZE iot_dashboard.silver_sensors ZORDER BY (timestamp);

-- This calculate table stats for all columns to ensure the optimizer can build the best plan
-- Statement 3

ANALYZE TABLE iot_dashboard.silver_sensors COMPUTE STATISTICS FOR ALL COLUMNS;

CREATE OR REPLACE TABLE hourly_summary_statistics
AS
SELECT user_id,
date_trunc('hour', timestamp) AS HourBucket,
AVG(num_steps)::float AS AvgNumStepsAcrossDevices,
AVG(calories_burnt)::float AS AvgCaloriesBurnedAcrossDevices,
AVG(miles_walked)::float AS AvgMilesWalkedAcrossDevices
FROM silver_sensors
GROUP BY user_id,date_trunc('hour', timestamp)
ORDER BY HourBucket;

-- Statement 4
-- Truncate bronze batch once successfully loaded
TRUNCATE TABLE bronze_sensors;
"""

# COMMAND ----------

serverless_client_t = DBSQLTransactionManager(warehouse_id = warehouse_id, mode="inferred_altered_tables", token=token, host_name=host_name) ## token=<optional>, host_name=<optional>verbose=True for print statements and other debugging messages

# COMMAND ----------

# DBTITLE 1,Submitting the Multi Statement Transaction to Serverless SQL Warehouse
"""
PARAMS: 
warehouse_id --> Required, the SQL warehouse to submit statements
mode -> selected_tables, inferred_altered_tables
token --> optional, will try to get one for the user
host_name --> optional, will try to infer same workspace url


execute_sql_transaction params: 
return_type --> "message", "last_results". "message" will return status of query chain. "last_result" will run all statements and return the last results of the final query in the chain
"""

result_df = serverless_client_t.execute_sql_transaction(sql_string = sqlString)

# COMMAND ----------

display(result_df)
