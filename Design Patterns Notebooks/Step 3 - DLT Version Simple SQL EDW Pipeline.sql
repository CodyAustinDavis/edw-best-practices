-- Databricks notebook source
-- MAGIC %md
-- MAGIC
-- MAGIC # This notebook generates a full data pipeline from databricks dataset - iot-stream
-- MAGIC
-- MAGIC #### Define the SQL - Add as a library to a DLT pipeline, and run the pipeline!
-- MAGIC
-- MAGIC ## This creates 2 tables: 
-- MAGIC
-- MAGIC <b> Database: </b> iot_dashboard
-- MAGIC
-- MAGIC <b> Tables: </b> silver_sensors, silver_users 
-- MAGIC
-- MAGIC <b> Params: </b> StartOver (Yes/No) - allows user to truncate and reload pipeline

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC
-- MAGIC ## This is built as a library for a Delta Live Tables pipeline

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Exhaustive list of all cloud_files STREAMING LIVE TABLE options
-- MAGIC https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-incremental-data.html#language-sql

-- COMMAND ----------

-- DBTITLE 1,Incrementally Ingest Source Data from Raw Files
--No longer need a separate copy into statement, you can use the Databricks Autoloader directly in SQL by using the cloud_files function
-- OPTIONALLY defined DDL in the table definition
CREATE OR REFRESH STREAMING LIVE TABLE bronze_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING,
CONSTRAINT has_device EXPECT (device_id IS NOT NULL) ON VIOLATION DROP ROW  ,
CONSTRAINT has_user EXPECT(user_id IS NOT NULL) ON VIOLATION DROP ROW,
CONSTRAINT has_data EXPECT(num_steps IS NOT NULL) -- with no violation rule, nothing happens, we just track quality in DLT
)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="create_timestamp,device_id,user_id",
"pipelines.trigger.interval"="1 hour")
AS 
SELECT 
id::bigint AS Id,
device_id::integer AS device_id,
user_id::integer AS user_id,
calories_burnt::decimal(10,2) AS calories_burnt, 
miles_walked::decimal(10,2) AS miles_walked, 
num_steps::decimal(10,2) AS num_steps,
timestamp::timestamp AS timestamp,
value AS value
FROM cloud_files("/databricks-datasets/iot-stream/data-device/", "json")
 -- First 2 params of cloud_files are always input file path and format, then rest are map object of optional params
-- To make incremental - Add STREAMING keyword before LIVE TABLE
;



-- COMMAND ----------

-- MAGIC %md
-- MAGIC
-- MAGIC ## Process Change data with updates or deletes 
-- MAGIC API Docs: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cdc.html
-- MAGIC
-- MAGIC
-- MAGIC ### Automatically store change as SCD 1 or SCD 2 Type changes
-- MAGIC
-- MAGIC SCD 1/2 Docs: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-cdc.html#language-sql

-- COMMAND ----------

-- DBTITLE 1,Incremental upsert data into target silver layer
-- Create and populate the target table.
CREATE OR REFRESH STREAMING LIVE TABLE silver_sensors
(
Id BIGINT GENERATED BY DEFAULT AS IDENTITY,
device_id INT,
user_id INT,
calories_burnt DECIMAL(10,2), 
miles_walked DECIMAL(10,2), 
num_steps DECIMAL(10,2), 
timestamp TIMESTAMP,
value STRING)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"quality"="silver",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="create_timestamp,device_id,user_id",
"pipelines.trigger.interval"="1 hour"
);

-- COMMAND ----------

-- DBTITLE 1,Actually run CDC Transformation Operation
APPLY CHANGES INTO
  LIVE.silver_sensors
FROM
  STREAM(LIVE.bronze_sensors) -- use STREAM to get change feed, use LIVE to get DAG source table
KEYS
  (user_id, device_id) -- Identical to the ON statement in MERGE, can be 1 of many keys
--APPLY AS DELETE WHEN
--  operation = "DELETE" --Need if you have a operation columnd that specifies "APPEND"/"UPDATE"/"DELETE" like true CDC data
SEQUENCE BY
  timestamp
COLUMNS * EXCEPT
   (Id) --For auto increment keys, exclude the updates cause you dont want to replace Ids of auto_id columns
--    Optionally exclude columns like metadata or operation types, by default, UPDATE * is the operation
STORED AS
  SCD TYPE 1 -- [SCD TYPE 2] will expire updated originals

-- COMMAND ----------

-- MAGIC %md 
-- MAGIC
-- MAGIC ## FULL REFRESH EXAMPLE - Ingest Full User Data Set Each Load

-- COMMAND ----------

-- DBTITLE 1,FulltIngest Raw User Data
CREATE OR REPLACE STREAMING LIVE TABLE silver_users
( -- REPLACE truncates the checkpoint each time and loads from scratch every time
userid BIGINT GENERATED BY DEFAULT AS IDENTITY,
gender STRING,
age INT,
height DECIMAL(10,2), 
weight DECIMAL(10,2),
smoker STRING,
familyhistory STRING,
cholestlevs STRING,
bp STRING,
risk DECIMAL(10,2),
update_timestamp TIMESTAMP,
CONSTRAINT has_user EXPECT (userid IS NOT NULL) ON VIOLATION DROP ROW
)
TBLPROPERTIES("delta.targetFileSize"="128mb",
"quality"="silver",
"pipelines.autoOptimize.managed"="true",
"pipelines.autoOptimize.zOrderCols"="userid",
"pipelines.trigger.interval"="1 day"
)
AS (SELECT 
userid::bigint AS userid,
gender AS gender,
age::integer AS age,
height::decimal(10,2) AS height, 
weight::decimal(10,2) AS weight,
smoker AS smoker,
familyhistory AS familyhistory,
cholestlevs AS cholestlevs,
bp AS bp,
risk::decimal(10,2) AS risk,
current_timestamp() AS update_timestamp
FROM cloud_files("/databricks-datasets/iot-stream/data-user/","csv", map( 'header', 'true'))
)
;
